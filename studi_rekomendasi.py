# -*- coding: utf-8 -*-
"""Studi Rekomendasi.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rVm42JjoCqzEnGHd97-nUSirtKiSZamB

# **Studi Rekomendasi : Anime Recommendetion dengan Content-Base Filtering**

Model rekomendasi yang dibuat menggunakan Content-Base Filtering. Sistem ini akan memberikan output top 10 rekomendasi anime berdasarkan rating, genre, dan nama.

## **Dataset**

Dataset yang digunakan ada 2(dua) yaitu anime.csv dan rating.csv yang bisa ditemukan di kaggle.com. Dataset ini memiliki memiliki beberapa atribut diantaranya :

**Anime.csv**


*   anime_id - unique id yang mengidentifikasikan sebuah anime
*   name - nama lengkap anime
*   genre - kategori anime
*   type - movie, TV, OVA, dan lain-lain
*   episodes - berapa banyak episode anime (1 jika movie)
*   rating - rata-rata rating anime kisaran 0-10 
*   members - jumlah member di sebuah komunitas grup anime

**Rating.csv**

*   user_id - non identifiable randomly generated user id.
*   anime_id - anime yang telah dinilai pengguna ini.
*   rating - rating dari 10 yang telah ditetapkan pengguna ini (-1 jika pengguna menontonnya tetapi tidak memberikan peringkat).

# **Import Library**
"""

import math
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import datetime
import seaborn as sns
import re
import warnings
from scipy import stats as st
import matplotlib.lines as mlines

warnings.filterwarnings('ignore')

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import sigmoid_kernel

"""# **Import File**

Disini saya akan mengimport file dari kaggle dan mendownloadnya langsung dataset _Anime Recommendation Database_ dalam sebuah file zip lalu unzip file tersebut ke dalam file content
"""

from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/

!kaggle datasets download -d CooperUnion/anime-recommendations-database

!unzip /content/anime-recommendations-database.zip

"""# **Membaca File dan explore data**

Setelah mengimport file, langkah selanjutnya saya akan membaca dataset anime.csv dan rating.csv lalu mencari tau missing value didalam dataset dan melakukan proses handling missing value.
"""

anime_df = pd.read_csv('/content/anime.csv')
anime_df

anime_df.info()

anime_df.isnull().sum()

rating_df = pd.read_csv('/content/rating.csv')

rating_df

rating_df.isnull().sum()

print(rating_df.info())

print("Anime missing values (%):\n")
print(round(anime_df.isnull().sum().sort_values(ascending=False)/len(anime_df.index),4)*100) 
print("="*30,"\n\nRating missing values (%):")
print(round(rating_df.isnull().sum().sort_values(ascending=False)/len(rating_df.index),4)*100)

# deleting anime dengan rating 0
anime_df=anime_df[~np.isnan(anime_df["rating"])]

# filling mode value untuk genre dan type
anime_df['genre'] = anime_df['genre'].fillna(
anime_df['genre'].dropna().mode().values[0])

anime_df['type'] = anime_df['type'].fillna(
anime_df['type'].dropna().mode().values[0])

#mengecek nilai null kembali
anime_df.isnull().sum()

"""Judul anime harus di bersihkan dulu agar bisa memberikan rekomendasi yang tepat."""

def nameCleaning(text):
    text = re.sub(r'&quot;', '', text)
    text = re.sub(r'.hack//', '', text)
    text = re.sub(r'&#039;', '', text)
    text = re.sub(r'A&#039;s', '', text)
    text = re.sub(r'I&#039;', 'I\'', text)
    text = re.sub(r'&amp;', 'and', text)
    
    return text

anime_df['name'] = anime_df['name'].apply(nameCleaning)

"""# **Merging DataFrame**

Setelah malakukan proses explore data, saya menggabungkan kedua file yakni anime.csv dan rating.csv kedalam satu dataframe baru dan mengganti beberapa atribut nama kolomnya
"""

animeMerging =pd.merge(anime_df,rating_df,on='anime_id',suffixes= ['', '_user'])
animeMerging = animeMerging.rename(columns={'name': 'anime_title', 'rating_user': 'user_rating'})
animeMerging.head()

# Membuat sebuah dataframe untuk jumlah peringkat
combine_rating = animeMerging.dropna(axis = 0, subset = ['anime_title'])
ratingCount = (combine_rating.
     groupby(by = ['anime_title'])['user_rating'].
     count().
     reset_index().rename(columns = {'rating': 'totalRatingCount'})
    [['anime_title', 'user_rating']]
    )


top10_animerating=ratingCount[['anime_title', 'user_rating']].sort_values(by = 'user_rating',ascending = False).head(10)
ax=sns.barplot(x="anime_title", y="user_rating", data=top10_animerating, palette="Dark2")
ax.set_xticklabels(ax.get_xticklabels(), fontsize=11, rotation=40, ha="right")
ax.set_title('Top 10 Anime Berdasarkan Jumlah Rating',fontsize = 22)
ax.set_xlabel('Anime',fontsize = 20) 
ax.set_ylabel('User Rating count', fontsize = 20)

animeMerging = animeMerging.merge(ratingCount, left_on = 'anime_title', right_on = 'anime_title', how = 'left')
animeMerging = animeMerging.rename(columns={'user_rating_x': 'user_rating', 'user_rating_y': 'totalratingcount'})
animeMerging

import plotly.graph_objects as go
labels = animeMerging['type'].value_counts().index
values = animeMerging['type'].value_counts().values
colors = ['blue', 'red', 'orange', 'green']
fig = go.Figure(data=[go.Pie(labels=labels,
                             values=values)])
fig.update_traces(hoverinfo='label+percent', textinfo='value', textfont_size=20,
                  marker=dict(colors=colors, line=dict(color='#000000', width=2)))

fig.update_layout(
    title={
        'text': "Media Streaming",
        'y':0.9,
        'x':0.5,
        'xanchor': 'center',
        'yanchor': 'top'})

fig.show()

nonull_anime=animeMerging.copy()
nonull_anime.dropna(inplace=True)
from collections import defaultdict

all_genres = defaultdict(int)

for genres in nonull_anime['genre']:
    for genre in genres.split(','):
        all_genres[genre.strip()] += 1
        
from wordcloud import WordCloud

genres_cloud = WordCloud(width=800, height=400, background_color='white', colormap='gnuplot').generate_from_frequencies(all_genres)
plt.imshow(genres_cloud, interpolation='bilinear')
plt.axis('off')

"""# **Mengecek nilai null setelah merging data**"""

animeMerging.isnull().sum()

"""## **Standarisasi Label Numerik**

Standardisasi adalah teknik transformasi yang paling umum digunakan dalam tahap persiapan pemodelan. Untuk fitur numerik. saya akan menggunakan teknik StandarScaler dari library Scikitlearn, StandardScaler melakukan proses standarisasi fitur dengan mengurangkan mean (nilai rata-rata) kemudian membaginya dengan standar deviasi untuk menggeser distribusi. StandardScaler menghasilkan distribusi dengan standar deviasi sama dengan 1 dan mean sama dengan 0
"""

column_int = anime_df.dtypes[anime_df.dtypes == 'int64'].keys()
column_int

# Memilih semua kolom dengan tipe data float
column_float = anime_df.dtypes[anime_df.dtypes == 'float64'].keys()
column_float

# Menyatukan semua kolom dengan tipe data numerik
column_numeric = list(column_int) + list(column_float)
column_numeric

from sklearn.preprocessing import MinMaxScaler

# Inisiasi minmaxscaler
scaler = MinMaxScaler()

# Melakukan standarisasi data
scaled = scaler.fit_transform(anime_df[column_numeric])

# Mengganti data numerik dengan data yang sudah
# di standarisasi
i=0
for column in column_numeric:
    anime_df[column] = scaled[:,i]
    i += 1

"""# **Membuat Model Rekomendasi**

Model yang digunakan merupakan content base filtering dengan TF-ID Vectorizer.
"""

tfv = TfidfVectorizer(min_df=3,  max_features=None, 
            strip_accents='unicode', analyzer='word',token_pattern=r'\w{1,}',
            ngram_range=(1, 3),
            stop_words = 'english')

# Filling NaNs with empty string
anime_df['genre'] = anime_df['genre'].fillna('')
genres_str = anime_df['genre'].str.split(',').astype(str)
tfv_matrix = tfv.fit_transform(genres_str)

tfv_matrix.shape

"""Scikit-learn menyediakan pairwise metrics yang berfungsi untuk representasi koleksi vektor yang padat dan jarang. Di sini fungsi perlu menetapkan 1 untuk anime yang direkomendasikan dan 0 untuk anime yang tidak direkomendasikan. Jadi penulis menggunakan kernel sigmoid"""

# Menghitung kesamaan menggunakan sigmoid_kernel
sig = sigmoid_kernel(tfv_matrix, tfv_matrix)

indices = pd.Series(anime_df.index, index=anime_df['name']).drop_duplicates()

"""# **Membuat Fungsi Rekomendasi**

Setalah membuat model, saya coba membuat fungsi give_rec yang dapat memberikan hasil rekomendasi top 10 anime 
"""

def anime_rec(title, sig=sig):
    # Mendapatkan indeks yang sesuai dengan original_title
    idx = indices[title]

    # Mendapatkan pasangan similarity scores 
    sig_scores = list(enumerate(sig[idx]))

    # Mengurutkan movie 
    sig_scores = sorted(sig_scores, key=lambda x: x[1], reverse=True)

    # Skor dari 10 film yang paling mirip
    sig_scores = sig_scores[1:11]

    # mengindeks film
    anime_indices = [i[0] for i in sig_scores]

    # Membuat top 10 film paling mirip
    return pd.DataFrame({'Nama Anime': anime_df['name'].iloc[anime_indices].values, 'Genre': anime_df['genre'].iloc[anime_indices].values})

print('Nama anime : ', animeMerging.anime_title.unique())

"""# **Call Function Recommendation**

Setelah membuat model rekomendasi dengan content-base filtering, saatnya mencoba model yang telah dibuat dengan memanggil fungsi anime_rec() dan menampilkan top list rekomendasi.
"""

print("Top 10 Anime Recommended: ")
anime_rec('One Piece')

"""**Evaluation**

Hasil rekomendasi yang didapatkan sebanyak 7 item yang relevan dari 10 item yang direkomendasikan sesuai dengan judul anime dan genre. Dengan menggunakan metriks precision didapatkan hasil presisinya sebagai berikut:

 P = 7 / 10

 P = 70%

Hasil presisi nya sebesar 70%. Ini berarti sistem rekomendasi yang dibuat sudah cukup baik dalam memberikan Top-N Recommendation

"""